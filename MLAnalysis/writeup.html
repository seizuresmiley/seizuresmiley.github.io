<html>

<head>
    <!-- Compiled and minified CSS -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/materialize/1.0.0/css/materialize.min.css">

    <!-- Compiled and minified JavaScript -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/materialize/1.0.0/js/materialize.min.js"></script>
    <title>MLSeiyuuAnalysis</title>
</head>

<body>
    <nav>
        <div class="nav-wrapper">
            <a href="#" class="brand-logo center">MLSA</a>
            <ul id="nav-mobile" class="left hide-on-med-and-down">
                <li><a href="https://github.com/seizuresmiley/ml-seiyuu-analysis">Github</a></li>
            </ul>
        </div>
    </nav>
    <div class="row">
        <h1 class="center">Writeup</h1>
        <h5 class="center">More Information on the Subject.</h5>
        <br><br>
        <div class="col s12 center">
            <h3>Technical Information</h3>
        </div>
    </div>
    <br><br>
    <div class="row">
        <div class="col s3 offset-s1">
            <h4 id="data">Data Collection</h4>
        </div>
        <div class="col s7">
            <p>
                Python is our language of choice for this project, used for both data collection and analysis.
                <br><br>
                At first, we sourced our list of usernames (AKA handles), from a community maintained list of seiyuus
                then compiled the data into a CSV file.
                <br><br>
                Twitter offers an API that we used to collect user information and tweets. What we learned that we can
                collect 200 tweets per 1 GET request.
                we used a thrid-party library named <a href="">tweepy</a> to simplify the process. Tweepy is a wrapper
                around Twitter's API, which also handles authentication and "paging" for us.
                <br><br>
                We can then write a script to read the csv file, then using the "GET statuses/user-timeine/" API endpoint,
                we would have a Tweet object with tweet contents and user information in a JSON format, with each request
                yielding 200 tweets. Unfortunately,due to API limitations
                imposed by Twitter, we cannot retrieve more than 3200 tweets without using their paid historical search API.
                However, not all users we researched would have 3200 tweets.
                <br><br>
                Then we can dump the data into a JSON file for later analysis. This is to prevent calling the API excessively. (this endpoint
                has a limit of 1500 requests per 15 minutes if autheticated using app auth.) It also makes sure that all users' information
                are pulled in a relatively small timeframe.
            </p>
        </div>
    </div>
    </div>
    
</body>

</html>